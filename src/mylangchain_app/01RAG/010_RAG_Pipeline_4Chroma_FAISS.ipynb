{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1F5lTDp5UPf0",
   "metadata": {
    "id": "1F5lTDp5UPf0"
   },
   "source": [
    "### 1) ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6",
   "metadata": {
    "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6"
   },
   "outputs": [],
   "source": [
    "#poetry add langchain_community chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55152049-e9e5-4952-8e19-409f58cf3ac9",
   "metadata": {
    "id": "55152049-e9e5-4952-8e19-409f58cf3ac9"
   },
   "source": [
    "### 2) OpenAI ì¸ì¦í‚¤ ì„¤ì •\n",
    "https://openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b76f68a8-4745-4377-8057-6090b87377d1",
   "metadata": {
    "id": "b76f68a8-4745-4377-8057-6090b87377d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a9abc5",
   "metadata": {},
   "source": [
    "##### Chroma ê°„ë‹¨í•œ ì˜ˆì œ\n",
    "* Chroma DBì— í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f0430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import TextLoader  # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings  # OpenAI ì„ë² ë”© ì‚¬ìš©\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # í…ìŠ¤íŠ¸ ë¶„í• ê¸°\n",
    "from langchain_chroma import Chroma  # ë²¡í„° DB (Chroma) ì‚¬ìš©\n",
    "\n",
    "\n",
    "# 2. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "DB_PATH = \"./db/chroma_db\"\n",
    "\n",
    "# 3. í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¬¸ì„œë¥¼ ë¶„í• í•˜ëŠ” í•¨ìˆ˜ ì •ì˜\n",
    "def load_and_split_text(file_path, splitter):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•œ í›„, ì„¤ì •ëœ Splitterë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ\n",
    "        splitter (RecursiveCharacterTextSplitter): í…ìŠ¤íŠ¸ ë¶„í• ê¸° ê°ì²´\n",
    "\n",
    "    Returns:\n",
    "        list: ë¶„í• ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        loader = TextLoader(file_path)  # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ\n",
    "        return loader.load_and_split(splitter)  # ë¶„í• í•˜ì—¬ ë°˜í™˜\n",
    "    except Exception as e:\n",
    "        print(f\" íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({file_path}): {e}\")\n",
    "        return []\n",
    "\n",
    "# 4. í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì • (600ì ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³ , 100ì ê²¹ì¹¨ í¬í•¨)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
    "\n",
    "# 5. ë‘ ê°œì˜ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ë° ë¶„í• \n",
    "split_doc1 = load_and_split_text(\"data/ai-terminology.txt\", text_splitter)\n",
    "split_doc2 = load_and_split_text(\"data/finance-terminology.txt\", text_splitter)\n",
    "\n",
    "# 6. ë¬¸ì„œ ê°œìˆ˜ ì¶œë ¥\n",
    "print(f\"AI ë¬¸ì„œ ê°œìˆ˜: {len(split_doc1)}\")\n",
    "print(f\"ê¸ˆìœµ ë¬¸ì„œ ê°œìˆ˜: {len(split_doc2)}\")\n",
    "\n",
    "# 7. ëª¨ë“  ë¬¸ì„œ í•©ì¹˜ê¸°\n",
    "all_documents = split_doc1 + split_doc2\n",
    "\n",
    "# 8. Chroma ë²¡í„° DB ìƒì„± ë° ì €ì¥\n",
    "try:\n",
    "    persist_db = Chroma.from_documents(\n",
    "        documents=all_documents,\n",
    "        embedding=OpenAIEmbeddings(),  # OpenAI Embeddings ì‚¬ìš©\n",
    "        persist_directory=DB_PATH,  # ë²¡í„° DB ì €ì¥ ìœ„ì¹˜ ì§€ì •\n",
    "        collection_name=\"my_vector_db\",  # ë°ì´í„°ë² ì´ìŠ¤ ì»¬ë ‰ì…˜ ì´ë¦„\n",
    "    )\n",
    "    print(\"Chroma ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ!\")\n",
    "except Exception as e:\n",
    "    print(f\" Chroma ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 9. ì €ì¥ëœ ë°ì´í„° í™•ì¸\n",
    "try:\n",
    "    retrieved_docs = persist_db.get()  # Chroma DBì—ì„œ ë°ì´í„° ì¡°íšŒ\n",
    "    print(f\" ì €ì¥ëœ ë²¡í„° ê°œìˆ˜: {len(retrieved_docs['ids'])}, íƒ€ì… {type(retrieved_docs['ids'])}\")\n",
    "except Exception as e:\n",
    "    print(f\" ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 10. ìœ ì‚¬ë„ ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜\n",
    "def search_query(query, k=2):\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ì ì…ë ¥(query)ì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜.\n",
    "\n",
    "    Args:\n",
    "        query (str): ê²€ìƒ‰í•  ë¬¸ì¥ (ì˜ˆ: \"Transformer ê°œë… ì„¤ëª…\")\n",
    "        k (int, optional): ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        None: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¶œë ¥\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = persist_db.similarity_search(query, k=k)  # ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "        print(f\"\\n [Query]: {query}\\n\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"ğŸ”¹ [Result {i+1}]: {doc.page_content[:300]}...\\n\")  # ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥\n",
    "    except Exception as e:\n",
    "        print(f\" ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 11. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "search_query(\"Transformer ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\", k=2)\n",
    "search_query(\"Hedge Fund ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜?\", k=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fbd63b",
   "metadata": {},
   "source": [
    "##### FAISS ê°„ë‹¨í•œ ì˜ˆì œ\n",
    "* FAISS DBì— í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e027701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_community\n",
    "#!pip install langchain_ollama\n",
    "#!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "218ba88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI ë¬¸ì„œ ê°œìˆ˜: 19\n",
      "ê¸ˆìœµ ë¬¸ì„œ ê°œìˆ˜: 12\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader  # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # í…ìŠ¤íŠ¸ ë¶„í• ê¸°\n",
    "from langchain_community.vectorstores import FAISS  # ë²¡í„° DB (FAISS) ì‚¬ìš©\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# 2. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "DB_PATH = \"./faiss_db\"\n",
    "\n",
    "# 3. í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¬¸ì„œë¥¼ ë¶„í• í•˜ëŠ” í•¨ìˆ˜ ì •ì˜\n",
    "def load_and_split_text(file_path, splitter):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•œ í›„, ì„¤ì •ëœ Splitterë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ\n",
    "        splitter (RecursiveCharacterTextSplitter): í…ìŠ¤íŠ¸ ë¶„í• ê¸° ê°ì²´\n",
    "\n",
    "    Returns:\n",
    "        list: ë¶„í• ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\" íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        loader = TextLoader(file_path, encoding=\"utf-8\")  # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ\n",
    "        return loader.load_and_split(splitter)  # ë¶„í• í•˜ì—¬ ë°˜í™˜\n",
    "    except Exception as e:\n",
    "        print(f\" íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({file_path}): {e}\")\n",
    "        return []\n",
    "\n",
    "# 4. í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì • (600ì ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³ , 100ì ê²¹ì¹¨ í¬í•¨)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=30)\n",
    "\n",
    "# 5. ë‘ ê°œì˜ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ë° ë¶„í• \n",
    "split_doc1 = load_and_split_text(\"../data/ai-terminology.txt\", text_splitter)\n",
    "split_doc2 = load_and_split_text(\"../data/finance-terminology.txt\", text_splitter)\n",
    "\n",
    "# 6. ë¬¸ì„œ ê°œìˆ˜ ì¶œë ¥\n",
    "print(f\"AI ë¬¸ì„œ ê°œìˆ˜: {len(split_doc1)}\")\n",
    "print(f\"ê¸ˆìœµ ë¬¸ì„œ ê°œìˆ˜: {len(split_doc2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c371fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ!\n",
      " ì €ì¥ëœ ë²¡í„° ê°œìˆ˜: 31\n",
      " ë²¡í„° ì°¨ì›: 1024\n",
      " ì¸ë±ìŠ¤ íƒ€ì…: <class 'faiss.swigfaiss.IndexFlatL2'>\n"
     ]
    }
   ],
   "source": [
    "# 7. ëª¨ë“  ë¬¸ì„œ í•©ì¹˜ê¸°\n",
    "all_documents = split_doc1 + split_doc2\n",
    "\n",
    "# 8. FAISS ë²¡í„° DB ìƒì„± ë° ì €ì¥\n",
    "try:\n",
    "    ollamaEmbeddings = OllamaEmbeddings(model=\"bge-m3:latest\")\n",
    "\n",
    "    # FAISS ë²¡í„° DB ìƒì„±\n",
    "    persist_db = FAISS.from_documents(\n",
    "        documents=all_documents,\n",
    "        embedding=ollamaEmbeddings,\n",
    "    )\n",
    "    \n",
    "    # ë¡œì»¬ ë””ìŠ¤í¬ì— ì €ì¥\n",
    "    if not os.path.exists(DB_PATH):\n",
    "        os.makedirs(DB_PATH)\n",
    "    persist_db.save_local(DB_PATH)\n",
    "    \n",
    "    print(\"FAISS ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ!\")\n",
    "except Exception as e:\n",
    "    print(f\" FAISS ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 9. ì €ì¥ëœ ë°ì´í„° í™•ì¸\n",
    "try:\n",
    "    # FAISSì—ì„œ ì¸ë±ìŠ¤ ì •ë³´ í™•ì¸\n",
    "    print(f\" ì €ì¥ëœ ë²¡í„° ê°œìˆ˜: {persist_db.index.ntotal}\")\n",
    "    print(f\" ë²¡í„° ì°¨ì›: {persist_db.index.d}\")\n",
    "    print(f\" ì¸ë±ìŠ¤ íƒ€ì…: {type(persist_db.index)}\")\n",
    "except Exception as e:\n",
    "    print(f\" ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7055bd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI ìš©ì–´ =================\n",
      "\n",
      " [Query]: Embedding ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\n",
      "\n",
      "[Result 1]: Embedding (ì„ë² ë”©)\n",
      "\n",
      "ì •ì˜: ë‹¨ì–´ë‚˜ ë¬¸ì¥ì„ ë²¡í„° ê³µê°„ì— ë§¤í•‘í•˜ì—¬ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ê²ƒë“¤ì´ ê°€ê¹Œì´ ìœ„ì¹˜í•˜ë„ë¡ í•˜ëŠ” ê¸°ë²•.\n",
      "ì˜ˆì‹œ: \"ê°•ì•„ì§€\"ì™€ \"ê³ ì–‘ì´\"ì˜ ë²¡í„° í‘œí˜„ì´ ìœ ì‚¬í•˜ê²Œ ìœ„ì¹˜í•¨.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ë²¡í„°í™”, ìì—°ì–´ ì²˜ë¦¬, ë”¥ëŸ¬ë‹\n",
      "\n",
      "Token (í† í°)...\n",
      "\n",
      "[Result 2]: ì •ì˜: FAISSëŠ” í˜ì´ìŠ¤ë¶ì—ì„œ ê°œë°œí•œ ê³ ì† ìœ ì‚¬ì„± ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, íŠ¹íˆ ëŒ€ê·œëª¨ ë²¡í„° ì§‘í•©ì—ì„œ ìœ ì‚¬ ë²¡í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ì˜ˆì‹œ: ìˆ˜ë°±ë§Œ ê°œì˜ ì´ë¯¸ì§€ ë²¡í„° ì¤‘ì—ì„œ ë¹„ìŠ·í•œ ì´ë¯¸ì§€ë¥¼ ë¹ ë¥´ê²Œ ì°¾ëŠ” ë° FAISSê°€ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "ì—°ê´€í‚¤ì›Œë“œ: ë²¡í„° ê²€ìƒ‰, ë¨¸ì‹ ëŸ¬ë‹, ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”\n",
      "\n",
      "Embedding (ì„ë² ë”©)...\n",
      "\n",
      "[Result 3]: ì •ì˜: ë³´ìƒì„ ê·¹ëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í–‰ë™ì„ í•™ìŠµí•˜ëŠ” AI ê¸°ë²•.\n",
      "ì˜ˆì‹œ: ì•ŒíŒŒê³ ê°€ ë°”ë‘‘ì—ì„œ ìµœì ì˜ ìˆ˜ë¥¼ ì°¾ê¸° ìœ„í•´ ê°•í™” í•™ìŠµì„ í™œìš©.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ë³´ìƒ ì‹œìŠ¤í…œ, ì •ì±… í•™ìŠµ, ê²Œì„ AI\n",
      "\n",
      "Prompt Engineering (í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§)\n",
      "\n",
      "ì •ì˜: AI ëª¨ë¸ì´ ì›í•˜ëŠ” ì¶œë ¥ì„ ìƒì„±í•˜ë„ë¡ ì…ë ¥ì„ ìµœì í™”í•˜ëŠ” ê¸°ìˆ .\n",
      "ì˜ˆì‹œ: \"í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜\"ë¼ëŠ” ì…ë ¥ì„ í†µí•´ ì§§ì€ ë‹µë³€ì„ ìœ ë„.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ìì—°ì–´ ìƒì„±, GPT, ì…ë ¥ ìµœì í™”...\n",
      "\n",
      "Finance ìš©ì–´ =================\n",
      "\n",
      " [Query]: Hedge Fund ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\n",
      "\n",
      "[Result 1]: Hedge Fund (í—¤ì§€í€ë“œ)\n",
      "\n",
      "ì •ì˜: ê³µê²©ì ì¸ íˆ¬ì ì „ëµì„ ì‚¬ìš©í•˜ì—¬ ë†’ì€ ìˆ˜ìµì„ ì¶”êµ¬í•˜ëŠ” íˆ¬ì í€ë“œ.\n",
      "ì˜ˆì‹œ: í—¤ì§€í€ë“œëŠ” ê³µë§¤ë„, ë ˆë²„ë¦¬ì§€ ë“± ë‹¤ì–‘í•œ ì „ëµì„ í™œìš©í•¨.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ê³ ìœ„í—˜ íˆ¬ì, ì ê·¹ì  ìš´ìš©, ë ˆë²„ë¦¬ì§€\n",
      "\n",
      "Asset Allocation (ìì‚° ë°°ë¶„)\n",
      "\n",
      "ì •ì˜: íˆ¬ì í¬íŠ¸í´ë¦¬ì˜¤ì—ì„œ ì£¼ì‹, ì±„ê¶Œ, í˜„ê¸ˆ ë“± ë‹¤ì–‘í•œ ìì‚°ì„ ë°°ë¶„í•˜ëŠ” ì „ëµ.\n",
      "ì˜ˆì‹œ: ì—°ë ¹ëŒ€ë³„ë¡œ ì ì ˆí•œ ìì‚° ë°°ë¶„ ì „ëµì´ í•„ìš”í•¨.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ë¦¬ìŠ¤í¬ ê´€ë¦¬, íˆ¬ì ì „ëµ, í¬íŠ¸í´ë¦¬ì˜¤...\n",
      "\n",
      "[Result 2]: Blue Chip Stocks (ìš°ëŸ‰ì£¼)\n",
      "\n",
      "ì •ì˜: ì‹œì¥ì—ì„œ ì•ˆì •ì ì´ê³  ì‹ ë¢°ë°›ëŠ” ëŒ€í˜• ê¸°ì—…ì˜ ì£¼ì‹.\n",
      "ì˜ˆì‹œ: ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ì™€ ì• í”Œì€ ëŒ€í‘œì ì¸ ë¸”ë£¨ì¹© ì£¼ì‹ì„.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ì¥ê¸° íˆ¬ì, ì•ˆì •ì„±, ëŒ€í˜•ì£¼\n",
      "\n",
      "Mutual Fund (ë®¤ì¶”ì–¼ í€ë“œ)\n",
      "\n",
      "ì •ì˜: ì—¬ëŸ¬ íˆ¬ììì˜ ìê¸ˆì„ ëª¨ì•„ ë‹¤ì–‘í•œ ìì‚°ì— íˆ¬ìí•˜ëŠ” í€ë“œ.\n",
      "ì˜ˆì‹œ: ë®¤ì¶”ì–¼ í€ë“œëŠ” ë¶„ì‚° íˆ¬ìë¡œ ìœ„í—˜ì„ ì¤„ì´ëŠ” ë° ë„ì›€ì„ ì¤Œ.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ê°„ì ‘ íˆ¬ì, í¬íŠ¸í´ë¦¬ì˜¤, í€ë“œ ë§¤ë‹ˆì €\n",
      "\n",
      "Hedge Fund (í—¤ì§€í€ë“œ)...\n",
      "\n",
      "[Result 3]: ETF (ìƒì¥ì§€ìˆ˜í€ë“œ)\n",
      "\n",
      "ì •ì˜: ì£¼ì‹ì²˜ëŸ¼ ê±°ë˜ë˜ëŠ” í€ë“œë¡œ, íŠ¹ì • ì§€ìˆ˜ë¥¼ ì¶”ì¢…í•¨.\n",
      "ì˜ˆì‹œ: S&P 500 ETFëŠ” ë¯¸êµ­ ëŒ€í˜•ì£¼ ì‹œì¥ì„ ë°˜ì˜í•¨.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ê°„ì ‘ íˆ¬ì, ë¶„ì‚° íˆ¬ì, ì§€ìˆ˜ íˆ¬ì\n",
      "\n",
      "Short Selling (ê³µë§¤ë„)\n",
      "\n",
      "ì •ì˜: ì£¼ê°€ í•˜ë½ì„ ì˜ˆìƒí•˜ê³  ì£¼ì‹ì„ ë¹Œë ¤ ë§¤ë„í•œ í›„, ê°€ê²©ì´ í•˜ë½í•˜ë©´ ì‹¼ ê°€ê²©ì— ë‹¤ì‹œ ë§¤ì…í•˜ì—¬ ì°¨ìµì„ ì–»ëŠ” íˆ¬ì ê¸°ë²•.\n",
      "ì˜ˆì‹œ: í—¤ì§€í€ë“œë“¤ì€ ì¢…ì¢… ê³µë§¤ë„ë¥¼ í™œìš©í•˜ì—¬ ìˆ˜ìµì„ ì°½ì¶œí•¨.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: í•˜ë½ ë² íŒ…, ìœ„í—˜ ê´€ë¦¬, ë ˆë²„ë¦¬ì§€...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10. ìœ ì‚¬ë„ ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜\n",
    "def search_query(query, k=5):\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ì ì…ë ¥(query)ì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜.\n",
    "\n",
    "    Args:\n",
    "        query (str): ê²€ìƒ‰í•  ë¬¸ì¥ (ì˜ˆ: \"Transformer ê°œë… ì„¤ëª…\")\n",
    "        k (int, optional): ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        None: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¶œë ¥\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = persist_db.similarity_search(query, k=k)  # ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "        print(f\"\\n [Query]: {query}\\n\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"[Result {i+1}]: {doc.page_content[:300]}...\\n\")  # ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥\n",
    "    except Exception as e:\n",
    "        print(f\" ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 11. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "print('AI ìš©ì–´ =================')\n",
    "search_query(\"Embedding ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\", k=3)\n",
    "print('Finance ìš©ì–´ =================')\n",
    "search_query(\"Hedge Fund ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\", k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4585881f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ì €ì¥ëœ FAISS DB ë¡œë“œ í…ŒìŠ¤íŠ¸\n",
      "==================================================\n",
      "FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ!\n",
      "ë¡œë“œëœ ë²¡í„° ê°œìˆ˜: 31\n",
      "\n",
      "ë¡œë“œëœ DB ê²€ìƒ‰ ê²°ê³¼: Few-shot Learning (í“¨ìƒ· í•™ìŠµ)\n",
      "\n",
      "ì •ì˜: ì ì€ ì–‘ì˜ ë°ì´í„°ë¡œë„ ìƒˆë¡œìš´ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” í•™ìŠµ ë°©ë²•.\n",
      "ì˜ˆì‹œ: ChatGPTê°€ ëª‡ ê°œì˜ ì˜ˆì œë§Œìœ¼ë¡œë„ ìƒˆë¡œìš´ ì§ˆë¬¸ì— ì ì ˆí•œ ë‹µì„ ìƒì„±í•¨.\n",
      "ì—°ê´€ í‚¤ì›Œë“œ: ìƒ˜í”Œ íš¨ìœ¨ì„±, ì „ì´ í•™ìŠµ, ë©”íƒ€ í•™ìŠµ\n",
      "\n",
      "Reinforcement Learning (ê°•í™” í•™ìŠµ)...\n"
     ]
    }
   ],
   "source": [
    "# 12. ì €ì¥ëœ FAISS DB ë¡œë“œ í…ŒìŠ¤íŠ¸\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ì €ì¥ëœ FAISS DB ë¡œë“œ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    ollamaEmbeddings = OllamaEmbeddings(model=\"bge-m3:latest\")\n",
    "    # ì €ì¥ëœ FAISS DB ë¡œë“œ\n",
    "    loaded_db = FAISS.load_local(\n",
    "        DB_PATH, \n",
    "        ollamaEmbeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    print(\"FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ!\")\n",
    "    \n",
    "    # ë¡œë“œëœ DBë¡œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "    print(f\"ë¡œë“œëœ ë²¡í„° ê°œìˆ˜: {loaded_db.index.ntotal}\")\n",
    "    \n",
    "    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "    test_results = loaded_db.similarity_search(\"í•™ìŠµì—ëŠ” ì–´ë–¤ê²ƒë“¤ì´ ìˆë‚˜ìš”?\", k=3)\n",
    "    print(f\"\\në¡œë“œëœ DB ê²€ìƒ‰ ê²°ê³¼: {test_results[0].page_content[:300]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"FAISS DB ë¡œë“œ ì˜¤ë¥˜: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e78988a",
   "metadata": {},
   "source": [
    "* [k-ìµœê·¼ì ‘ì´ì›ƒ ì•Œê³ ë¦¬ì¦˜](https://ko.wikipedia.org/wiki/K-%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98)\n",
    "* FAISSì—ì„œ similarity_search_with_score() í•¨ìˆ˜ì˜ ì ìˆ˜ëŠ” ì‹¤ì œë¡œëŠ” **ê±°ë¦¬(distance)**ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "    * ë‚®ì€ ì ìˆ˜ = ë” ìœ ì‚¬í•¨ (ê±°ë¦¬ê°€ ê°€ê¹Œì›€)\n",
    "    : Result 1 (Score: 0.3795) â†’ ë” ìœ ì‚¬í•œ ê²°ê³¼\n",
    "    * ë†’ì€ ì ìˆ˜ = ëœ ìœ ì‚¬í•¨ (ê±°ë¦¬ê°€ ë©€ìŒ)\n",
    "    : Result 2 (Score: 0.4341) â†’ ëœ ìœ ì‚¬í•œ ê²°ê³¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13183274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def search_with_score(query, k=5):\n",
    "    \"\"\"\n",
    "    ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = persist_db.similarity_search_with_score(query, k=k)\n",
    "        print(f\"\\n [Query]: {query}\\n\")\n",
    "        for i, (doc, score) in enumerate(results):\n",
    "            print(f\"[Result {i+1}] (Score: {score:.4f}):\")\n",
    "            print(f\"{doc.page_content[:500]}...\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\" ì ìˆ˜ ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "search_with_score(\"í•™ìŠµì—ëŠ” ì–´ë–¤ê²ƒë“¤ì´ ìˆë‚˜ìš”?\", k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec4fa42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FAISS ì¸ë±ìŠ¤ ìƒì„¸ ì •ë³´\n",
      "==================================================\n",
      "ì¸ë±ìŠ¤ íƒ€ì…: IndexFlatL2\n",
      "ì´ ë²¡í„° ìˆ˜: 31\n",
      "ë²¡í„° ì°¨ì›: 1024\n",
      "í›ˆë ¨ ì—¬ë¶€: True\n",
      "ë©”íŠ¸ë¦­ íƒ€ì…: 1\n",
      "ì¶”ì • ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 0.12 MB\n"
     ]
    }
   ],
   "source": [
    "# 14. FAISS ì¸ë±ìŠ¤ ì •ë³´ ì¶œë ¥\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FAISS ì¸ë±ìŠ¤ ìƒì„¸ ì •ë³´\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    index = persist_db.index\n",
    "    print(f\"ì¸ë±ìŠ¤ íƒ€ì…: {type(index).__name__}\")\n",
    "    print(f\"ì´ ë²¡í„° ìˆ˜: {index.ntotal}\")\n",
    "    print(f\"ë²¡í„° ì°¨ì›: {index.d}\")\n",
    "    print(f\"í›ˆë ¨ ì—¬ë¶€: {index.is_trained}\")\n",
    "    print(f\"ë©”íŠ¸ë¦­ íƒ€ì…: {index.metric_type}\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ì¶”ì •)\n",
    "    memory_usage = index.ntotal * index.d * 4 / (1024 * 1024)  # 4 bytes per float, MB ë‹¨ìœ„\n",
    "    print(f\"ì¶”ì • ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "faiss-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
