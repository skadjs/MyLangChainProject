{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a0f66cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì œ 1: ì¹´í˜ ë©”ë‰´ ë„êµ¬ í˜¸ì¶œ ì²´ì¸ êµ¬í˜„\n",
    "\n",
    "import re\n",
    "import os\n",
    "from textwrap import dedent\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig, chain\n",
    "\n",
    "from langchain_upstage import UpstageEmbeddings, ChatUpstage\n",
    "from langchain_community.tools import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce343c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:2])\n",
    "\n",
    "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "print(UPSTAGE_API_KEY[30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9178d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. ì¹´í˜ ë©”ë‰´ ë°ì´í„° ë¡œë“œ ë° ë²¡í„° DB êµ¬ì¶•\n",
    "def create_cafe_vector_db():\n",
    "    # ì¹´í˜ ë©”ë‰´ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œ\n",
    "    loader = TextLoader(\"../data/cafe_menu_data.txt\", encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # ë©”ë‰´ í•­ëª©ë³„ë¡œ ë¶„í• \n",
    "    def split_menu_items(document):\n",
    "        pattern = r'(\\d+\\.\\s.*?)(?=\\n\\n\\d+\\.|$)'\n",
    "        menu_items = re.findall(pattern, document.page_content, re.DOTALL)\n",
    "        \n",
    "        menu_documents = []\n",
    "        for i, item in enumerate(menu_items, 1):\n",
    "            # ë©”ë‰´ ì´ë¦„ ì¶”ì¶œ\n",
    "            menu_name = item.split('\\n')[0].split('.', 1)[1].strip()\n",
    "            \n",
    "            menu_doc = Document(\n",
    "                page_content=item.strip(),\n",
    "                metadata={\n",
    "                    \"source\": document.metadata['source'],\n",
    "                    \"menu_number\": i,\n",
    "                    \"menu_name\": menu_name\n",
    "                }\n",
    "            )\n",
    "            menu_documents.append(menu_doc)\n",
    "        \n",
    "        return menu_documents\n",
    "    \n",
    "    # ë©”ë‰´ í•­ëª© ë¶„ë¦¬ ì‹¤í–‰\n",
    "    menu_documents = []\n",
    "    for doc in documents:\n",
    "        menu_documents += split_menu_items(doc)\n",
    "    \n",
    "    # ì„ë² ë”© ëª¨ë¸ ì„¤ì •\n",
    "    #embeddings_model = OllamaEmbeddings(model=\"bge-m3:latest\")\n",
    "    embeddings_model = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "    \n",
    "    # FAISS ì¸ë±ìŠ¤ ìƒì„±\n",
    "    cafe_db = FAISS.from_documents(\n",
    "        documents=menu_documents, \n",
    "        embedding=embeddings_model\n",
    "    )\n",
    "    \n",
    "    # FAISS ì¸ë±ìŠ¤ ì €ì¥\n",
    "    cafe_db.save_local(\"../db/cafe_db\")\n",
    "    \n",
    "    return cafe_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "928297f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.tools.structured.StructuredTool'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. ë„êµ¬ ì •ì˜\n",
    "# ì›¹ ê²€ìƒ‰ ë„êµ¬\n",
    "@tool\n",
    "def tavily_search_func(query: str) -> str:\n",
    "    \"\"\"Searches the internet for information that does not exist in the database or for the latest information.\"\"\"\n",
    "    tavily_search = TavilySearchResults(max_results=2)\n",
    "    docs = tavily_search.invoke(query)\n",
    "    \n",
    "    formatted_docs = \"\\n---\\n\".join([\n",
    "        f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "        for doc in docs\n",
    "    ])\n",
    "    \n",
    "    if len(formatted_docs) > 0:\n",
    "        return formatted_docs\n",
    "    \n",
    "    return \"ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "print(type(tavily_search_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39f7469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ìœ„í‚¤í”¼ë””ì•„ ê²€ìƒ‰ ë„êµ¬\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "def wiki_search_and_summarize(input_data: dict):\n",
    "    wiki_loader = WikipediaLoader(query=input_data[\"query\"], load_max_docs=2, lang=\"ko\")\n",
    "    wiki_docs = wiki_loader.load()\n",
    "    \n",
    "    formatted_docs = [\n",
    "        f'<Document source=\"{doc.metadata[\"source\"]}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "        for doc in wiki_docs\n",
    "    ]\n",
    "    \n",
    "    return formatted_docs\n",
    "\n",
    "class WikiSummarySchema(BaseModel):\n",
    "    \"\"\"Input schema for Wikipedia search.\"\"\"\n",
    "    query: str = Field(..., description=\"The query to search for in Wikipedia\")\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize the following text in a concise manner:\\n\\n{context}\\n\\nSummary:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a47bacc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solar-pro\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# LLM ëª¨ë¸ \n",
    "# llm = ChatOpenAI(\n",
    "#     #api_key=OPENAI_API_KEY,\n",
    "#     base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "#     model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "#     temperature=0.7\n",
    "# )\n",
    "llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5,\n",
    ")\n",
    "print(llm.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45789a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.tools.structured.StructuredTool'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p_/_c9chpr14vxg3hvpscn52m740000gn/T/ipykernel_23356/1575251558.py:6: LangChainBetaWarning: This API is in beta and may change in the future.\n",
      "  wiki_summary = summary_chain.as_tool(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "summary_chain = (\n",
    "    {\"context\": RunnableLambda(wiki_search_and_summarize)}\n",
    "    | summary_prompt | llm\n",
    ")\n",
    "\n",
    "wiki_summary = summary_chain.as_tool(\n",
    "    name=\"wiki_summary\",\n",
    "    description=dedent(\"\"\"\n",
    "        Use this tool when you need to search for information on Wikipedia.\n",
    "        It searches for Wikipedia articles related to the user's query and returns\n",
    "        a summarized text. This tool is useful when general knowledge\n",
    "        or background information is required.\n",
    "    \"\"\"),\n",
    "    args_schema=WikiSummarySchema\n",
    ")\n",
    "print(type(wiki_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1f99bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.tools.structured.StructuredTool'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ì¹´í˜ ë©”ë‰´ ê²€ìƒ‰ ë„êµ¬\n",
    "@tool\n",
    "def db_search_cafe_func(query: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Securely retrieve and access authorized cafe menu information from the encrypted database.\n",
    "    Use this tool only for cafe menu-related queries to maintain data confidentiality.\n",
    "    \"\"\"\n",
    "    #embeddings_model = OllamaEmbeddings(model=\"bge-m3:latest\")\n",
    "    embeddings_model = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "\n",
    "    cafe_db = FAISS.load_local(\n",
    "        \"../db/cafe_db\", \n",
    "        embeddings_model, \n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    \n",
    "    docs = cafe_db.similarity_search(query, k=2)\n",
    "    if len(docs) > 0:\n",
    "        return docs\n",
    "    \n",
    "    return [Document(page_content=\"ê´€ë ¨ ì¹´í˜ ë©”ë‰´ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")]\n",
    "print(type(db_search_cafe_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ccfc362",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. ë„êµ¬ë¥¼ LLMì— ë°”ì¸ë”©\n",
    "tools = [tavily_search_func, wiki_summary, db_search_cafe_func]\n",
    "llm_with_tools = llm.bind_tools(tools=tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4a663d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. ê°„ë‹¨í•œ ë„êµ¬ í˜¸ì¶œ ì²´ì¸ êµ¬í˜„\n",
    "@chain\n",
    "def cafe_search_chain(user_input: str, config: RunnableConfig):\n",
    "    # ì²« ë²ˆì§¸ LLM í˜¸ì¶œë¡œ ë„êµ¬ ì‚¬ìš© ê²°ì •\n",
    "    ai_msg = llm_with_tools.invoke(user_input, config=config)\n",
    "    \n",
    "    # ë„êµ¬ ì‹¤í–‰\n",
    "    tool_msgs = []\n",
    "    for tool_call in ai_msg.tool_calls:\n",
    "        print(f\"{tool_call['name']}: \\n{tool_call}\")\n",
    "        print(\"-\"*100)\n",
    "        if tool_call[\"name\"] == \"tavily_search_func\":\n",
    "            tool_message = tavily_search_func.invoke(tool_call, config=config)\n",
    "            tool_msgs.append(tool_message)\n",
    "        elif tool_call[\"name\"] == \"wiki_summary\":\n",
    "            tool_message = wiki_summary.invoke(tool_call, config=config)\n",
    "            tool_msgs.append(tool_message)\n",
    "        elif tool_call[\"name\"] == \"db_search_cafe_func\":\n",
    "            tool_message = db_search_cafe_func.invoke(tool_call, config=config)\n",
    "            tool_msgs.append(tool_message)\n",
    "    \n",
    "    # ìµœì¢… ë‹µë³€ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸\n",
    "    final_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful cafe assistant. Provide accurate information based on the search results.\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "        (\"ai\", ai_msg.content if ai_msg.content else \"ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ë¥¼ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤.\"),\n",
    "        (\"human\", \"ê²€ìƒ‰ ê²°ê³¼: {tool_results}\")\n",
    "    ])\n",
    "    \n",
    "    # ë„êµ¬ ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "    tool_results_str = \"\\n\\n\".join([str(msg.content) for msg in tool_msgs])\n",
    "    \n",
    "    # ìµœì¢… ë‹µë³€ ìƒì„±\n",
    "    final_chain = final_prompt | llm\n",
    "    return final_chain.invoke({\n",
    "        \"user_input\": user_input,\n",
    "        \"tool_results\": tool_results_str\n",
    "    }, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff0be59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¹´í˜ ë©”ë‰´ ë²¡í„° DBê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "db_search_cafe_func: \n",
      "{'name': 'db_search_cafe_func', 'args': {'query': 'ì•„ë©”ë¦¬ì¹´ë…¸ ê°€ê²© ë° ìœ ë˜'}, 'id': 'chatcmpl-tool-ed04266c457b480589e8ead1b5278dff', 'type': 'tool_call'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "wiki_summary: \n",
      "{'name': 'wiki_summary', 'args': {'query': 'ì•„ë©”ë¦¬ì¹´ë…¸ ìœ ë˜'}, 'id': 'chatcmpl-tool-b37666f918ae42e383baa4b51e12a17a', 'type': 'tool_call'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tavily_search_func: \n",
      "{'name': 'tavily_search_func', 'args': {'query': 'ìµœê·¼ ì¸ê¸° ìˆëŠ” ì„œìš¸ ì¹´í˜ ì¶”ì²œ'}, 'id': 'chatcmpl-tool-4bad011ac707486ba284e9a09638509b', 'type': 'tool_call'}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p_/_c9chpr14vxg3hvpscn52m740000gn/T/ipykernel_23356/35227420.py:6: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily_search = TavilySearchResults(max_results=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì§ˆë¬¸: ì•„ë©”ë¦¬ì¹´ë…¸ì˜ ê°€ê²©ì€ ì–¼ë§ˆì¸ê°€ìš”? ì•„ë©”ë¦¬ì¹´ë…¸ì˜ ìœ ë˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”? ê·¸ë¦¬ê³  ìµœê·¼ì— ê°€ì¥ ì¸ê¸° ìˆëŠ” ì„œìš¸ì— ìˆëŠ” ì¹´í˜ë„ ì¶”ì²œí•´ ì£¼ì„¸ìš”.\n",
      "ë‹µë³€: ### 1. ì•„ë©”ë¦¬ì¹´ë…¸ ê°€ê²©  \n",
      "- **í•« ì•„ë©”ë¦¬ì¹´ë…¸**: â‚©4,500  \n",
      "- **ì•„ì´ìŠ¤ ì•„ë©”ë¦¬ì¹´ë…¸**: â‚©4,500  \n",
      "(ì œê³µëœ ì¹´í˜ ë©”ë‰´ ë°ì´í„° ê¸°ì¤€)\n",
      "\n",
      "---\n",
      "\n",
      "### 2. ì•„ë©”ë¦¬ì¹´ë…¸ì˜ ìœ ë˜  \n",
      "- **ê¸°ì›**: 20ì„¸ê¸° ì´ˆ ì´íƒˆë¦¬ì•„ì—ì„œ ìœ ë˜í–ˆë‹¤ëŠ” ì„¤ì´ ê°€ì¥ ìœ ë ¥í•©ë‹ˆë‹¤.  \n",
      "  - \"Americano\"ëŠ” ë¯¸êµ­ì¸ë“¤ì´ ìœ ëŸ½ì˜ ì—ìŠ¤í”„ë ˆì†Œë¥¼ ìì‹ ì˜ ì»¤í”¼ ë¬¸í™”ì— ë§ê²Œ **ëœ¨ê±°ìš´ ë¬¼ë¡œ í¬ì„**í•´ ë§ˆì‹  ë°ì„œ ì´ë¦„ì´ ë¶™ì—ˆìŠµë‹ˆë‹¤.  \n",
      "  - 2ì°¨ ì„¸ê³„ëŒ€ì „ ë‹¹ì‹œ ë¯¸êµ° ë³‘ì‚¬ë“¤ì´ ì´íƒˆë¦¬ì•„ì˜ ì—ìŠ¤í”„ë ˆì†Œë¥¼ í¬ì„í•´ ë§ˆì‹  ê²ƒì´ ì‹œì´ˆë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.  \n",
      "- **ì–´ì›**: ì˜ì–´ \"American\"(ë¯¸êµ­ì¸)ì—ì„œ íŒŒìƒë˜ì—ˆìœ¼ë©°, ì›ë‘ ë³¸ì—°ì˜ ë§›ì„ ê°•ì¡°í•˜ëŠ” í´ë˜ì‹í•œ ë¸”ë™ ì»¤í”¼ë¡œ ë°œì „í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. ì„œìš¸ ì¸ê¸° ì¹´í˜ ì¶”ì²œ (2024ë…„ ê¸°ì¤€)  \n",
      "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¢…í•©í•´ ìµœê·¼ ì¸ê¸° ìˆëŠ” ì¹´í˜ë¥¼ ì„ ì •í–ˆìŠµë‹ˆë‹¤:  \n",
      "\n",
      "#### ğŸ† **1ìœ„: ì²œìƒê°€ì˜¥ (ë‚™ì‚° ì„±ê³½ ê·¼ì²˜)**  \n",
      "- **íŠ¹ì§•**:  \n",
      "  - ë‚™ì‚° ì „ë§ê³¼ ì–´ìš°ëŸ¬ì§„ íë§ ê³µê°„.  \n",
      "  - ë®¤ì§€ì—„ CAFEë¡œ ë¦¬ëª¨ë¸ë§ë˜ì–´ ìŒë£Œ êµ¬ë§¤ ì‹œ ë°•ë¬¼ê´€ ì…ì¥ ê°€ëŠ¥.  \n",
      "  - ì‚°ë¯¸ ì—†ëŠ” ê³ ì†Œí•œ ì»¤í”¼ì™€ ìŠ¤ì½˜/ë² ì´ì»¤ë¦¬ ë©”ë‰´ê°€ ì¸ê¸°.  \n",
      "- **ì˜ì—…ì‹œê°„**: ì›”-ëª© 10:00 ~ 21:50 / ê¸ˆ-ì¼ 10:00 ~ 22:50  \n",
      "\n",
      "#### ğŸŒ¿ **2ìœ„: ì‚¬ìœ  (í•œë‚¨ë™)**  \n",
      "- **íŠ¹ì§•**:  \n",
      "  - 5ì¸µ ë³µí•©ë¬¸í™”ê³µê°„. ì¸µë§ˆë‹¤ ë‹¤ë¥¸ í…Œë§ˆ(ë¯¸ë””ì–´ì•„íŠ¸, ê°¤ëŸ¬ë¦¬, ìŠ¤ì¹´ì´ ë¼ìš´ì§€).  \n",
      "  - ì²œì°½ê³¼ ì‹ë¬¼ë¡œ í‰í™”ë¡œìš´ ë¶„ìœ„ê¸°. ì»¤í”¼ ì–‘ì€ ì ì§€ë§Œ ë¡œìŠ¤íŒ… ì§ì ‘ ì§„í–‰.  \n",
      "- **ì˜ì—…ì‹œê°„**: ë§¤ì¼ 11:00 ~ 22:00  \n",
      "\n",
      "#### ğŸ¶ **3ìœ„: êµ¬ìš±í¬ì”¨ (ì„œìš¸ìˆ²)**  \n",
      "- **íŠ¹ì§•**:  \n",
      "  - ë°˜ë ¤ë™ë¬¼ ë™ë°˜ ê°€ëŠ¥. íŒŒìŠ¤í…”í†¤ ì¸í…Œë¦¬ì–´ì™€ í¬í† ì¡´(ê·¸ë„¤) ì¸ê¸°.  \n",
      "  - ì§ì ‘ ë¡œìŠ¤íŒ…í•œ ì»¤í”¼ì™€ ë‹¤ì–‘í•œ ë² ì´ì»¤ë¦¬ ë©”ë‰´.  \n",
      "- **ì˜ì—…ì‹œê°„**: ë§¤ì¼ 12:00 ~ 22:00  \n",
      "\n",
      "#### ğŸŒƒ **4ìœ„: ë·°í´ëœì¦ˆ (ì†¡ë¦¬ë‹¨ê¸¸)**  \n",
      "- **íŠ¹ì§•**:  \n",
      "  - ì£¼íƒ ê°œì¡° ì¹´í˜ë¡œ ìš°ë“œ ì¸í…Œë¦¬ì–´ì™€ ìì—° ì¡°ë§.  \n",
      "  - ì‹œê·¸ë‹ˆì²˜ \"ìŠ¤ì›¨ë´ ì»¤í”¼\"ì™€ ë‹¬ì½¤í•œ ìŠ¤ìœ—ë¼ë–¼ ì¶”ì²œ.  \n",
      "- **ì˜ì—…ì‹œê°„**: ë§¤ì¼ 12:00 ~ 22:00 (ë¼ìŠ¤íŠ¸ ì˜¤ë” 21:20)  \n",
      "\n",
      "> â€» ì°¸ê³ : ì¸ê¸° ì¹´í˜ëŠ” ì‹œì¦Œë³„ë¡œ ë³€ë™ë  ìˆ˜ ìˆìœ¼ë‹ˆ ë°©ë¬¸ ì „ ì˜ì—…ì‹œê°„ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.  \n",
      "\n",
      "--- \n",
      "\n",
      "ë„ì›€ì´ ë˜ì…¨ë‹¤ë©´ ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì„ ì•Œë ¤ì£¼ì„¸ìš”! â˜•\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸\n",
    "if __name__ == \"__main__\":\n",
    "    # ë²¡í„° DB ìƒì„± (ìµœì´ˆ 1íšŒë§Œ ì‹¤í–‰)\n",
    "    try:\n",
    "        create_cafe_vector_db()\n",
    "        print(\"ì¹´í˜ ë©”ë‰´ ë²¡í„° DBê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ë²¡í„° DB ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    # ì§ˆë¬¸ì— ë‹µë³€    \n",
    "    query = \"ì•„ë©”ë¦¬ì¹´ë…¸ì˜ ê°€ê²©ì€ ì–¼ë§ˆì¸ê°€ìš”? ì•„ë©”ë¦¬ì¹´ë…¸ì˜ ìœ ë˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”? ê·¸ë¦¬ê³  ìµœê·¼ì— ê°€ì¥ ì¸ê¸° ìˆëŠ” ì„œìš¸ì— ìˆëŠ” ì¹´í˜ë„ ì¶”ì²œí•´ ì£¼ì„¸ìš”.\"\n",
    "    #query = \"ìµœê·¼ì— ê°€ì¥ ì¸ê¸° ìˆëŠ” ì„œìš¸ì— ìˆëŠ” ì¹´í˜ë¥¼ ì¶”ì²œí•´ ì£¼ì„¸ìš”.\"\n",
    "    response = cafe_search_chain.invoke(query)\n",
    "    \n",
    "    print(\"ì§ˆë¬¸:\", query)\n",
    "    print(\"ë‹µë³€:\", response.content)                                      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
