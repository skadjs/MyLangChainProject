{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8fa7ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello LangChain\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello LangChain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83669776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb097a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-pr\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "#load_dotenv(dotenv_path='.env')\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#print(OPENAI_API_KEY[:10])\n",
    "print(OPENAI_API_KEY[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef09a323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# prompt + llm + output \n",
    "\n",
    "# prompt: (ì‰½ê²Œ ë§í•´) AIì—ê²Œ í•˜ëŠ” ì§ˆë¬¸\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤.\") , # AIì—ê²Œ ì—­í• ì„ ì£¼ëŠ” ê²ƒ.\n",
    "     (\"user\", \"{input}\") ]\n",
    ")\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6c47481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "System: ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤.\n",
      "Human: íŒŒì´ì¬ì€ ë¬´ì—‡ì¸ê°€ìš”? ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”\n"
     ]
    }
   ],
   "source": [
    "prompt_text = prompt.format(input=\"íŒŒì´ì¬ì€ ë¬´ì—‡ì¸ê°€ìš”? ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”\")\n",
    "print(type(prompt_text))\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c61ba15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_openai.chat_models.base.ChatOpenAI'>\n",
      "client=<openai.resources.chat.completions.completions.Completions object at 0x10f7439a0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10f7ec280> root_client=<openai.OpenAI object at 0x10f743e20> root_async_client=<openai.AsyncOpenAI object at 0x118390eb0> model_name='openai/gpt-oss-120b' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "#llm = ChatOpenAI(api_key=OPENAI_API_KEY, model_name=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# GroqëŠ” ChatGPTì™€ í˜¸í™˜ -> ChatGPT ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥\n",
    "# Groq APIë¥¼ ì‚¬ìš©í•˜ëŠ” ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ëª¨ë¸ ê°€ì ¸ì™€ì„œ ìƒì„±)\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    #model=\"meta-llama/llama-4-scout-17b-16e-instruct\", # ì‚¬ìš© ëª¨ë¸: llama-4-scout\n",
    "    #model=\"moonshotai/kimi-k2-instruct-0905\", # ì‚¬ìš© ëª¨ë¸: kimi-k2\n",
    "    model=\"openai/gpt-oss-120b\", # ì‚¬ìš© ëª¨ë¸: gpt-oss-120b\n",
    "    temperature=0.7\n",
    ")\n",
    "print(type(llm)) # íƒ€ì…: ChatOpenAI\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c3ff864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤ë¥˜ ë°œìƒ: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n"
     ]
    }
   ],
   "source": [
    "# ìƒì„±í•œ ëª¨ë¸ì— ì§ˆë¬¸í•˜ê¸°(prompt ì…ë ¥)\n",
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    print(type(response))\n",
    "    print(\"ì‘ë‹µ:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfc67af",
   "metadata": {},
   "source": [
    "# LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2e4a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd77300d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "first=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]) middle=[ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x10f7439a0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10f7ec280>, root_client=<openai.OpenAI object at 0x10f743e20>, root_async_client=<openai.AsyncOpenAI object at 0x118390eb0>, model_name='openai/gpt-oss-120b', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')] last=StrOutputParser()\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "print(type(chain))\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ee6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"input\":\"LangChainì´ ë¬´ì—‡ì¸ê°€ìš”?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e647dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "## LangChainì´ë€?\n",
      "\n",
      "**LangChain**ì€ **ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)**ì„ í™œìš©í•œ **ì• í”Œë¦¬ì¼€ì´ì…˜**ì„ ë³´ë‹¤ ì‰½ê²Œ ì„¤ê³„Â·êµ¬í˜„Â·ë°°í¬í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” **í”„ë ˆì„ì›Œí¬**ì´ì **íˆ´í‚·**ì…ë‹ˆë‹¤. ì£¼ë¡œ íŒŒì´ì¬(Python)ê³¼ ìë°”ìŠ¤í¬ë¦½íŠ¸(TypeScript/Node.js) í™˜ê²½ì—ì„œ ì‚¬ìš©ë˜ë©°, LLMì„ ë‹¨ìˆœíˆ â€œí…ìŠ¤íŠ¸ë¥¼ ìƒì„±â€í•˜ëŠ” ìˆ˜ì¤€ì„ ë„˜ì–´ **ë³µí•©ì ì¸ ì›Œí¬í”Œë¡œìš°**ì™€ **ì™¸ë¶€ ì‹œìŠ¤í…œ ì—°ë™**ê¹Œì§€ í¬ê´„í•˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## í•µì‹¬ ê°œë… & êµ¬ì„± ìš”ì†Œ\n",
      "\n",
      "| êµ¬ì„± ìš”ì†Œ | ì„¤ëª… | ì£¼ìš” ì—­í•  |\n",
      "|-----------|------|-----------|\n",
      "| **Chains** | ì—¬ëŸ¬ LLM í˜¸ì¶œÂ·í”„ë¡¬í”„íŠ¸Â·íˆ´ì„ ìˆœì°¨Â·ì¡°ê±´ë¶€ë¡œ ì—°ê²°í•œ íŒŒì´í”„ë¼ì¸ | ë³µì¡í•œ ë¡œì§ì„ ë‹¨ê³„ë³„ë¡œ ë‚˜ëˆ„ì–´ ì¬ì‚¬ìš© ê°€ëŠ¥ |\n",
      "| **Agents** | ëª©í‘œ(Goal)ì™€ íˆ´(tool) ì§‘í•©ì„ ë°›ì•„, LLMì´ â€œì–´ë–¤ íˆ´ì„ ì–¸ì œ ì‚¬ìš©í• ì§€â€ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•˜ë„ë¡ í•˜ëŠ” êµ¬ì¡° | ë™ì  ì˜ì‚¬ê²°ì •Â·íˆ´ í˜¸ì¶œ(ê²€ìƒ‰, DB, API ë“±) |\n",
      "| **Prompt Templates** | í”„ë¡¬í”„íŠ¸ë¥¼ ë³€ìˆ˜í™”Â·ì¬ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ ë§Œë“  í…œí”Œë¦¿ | ì¼ê´€ëœ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬Â·ë‹¤ì–‘í•œ ì…ë ¥ì— ë§ì¶¤ |\n",
      "| **Memory** | ëŒ€í™”Â·ì„¸ì…˜ì˜ ìƒíƒœë¥¼ ì €ì¥Â·ì¡°íšŒí•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ | ì»¨í…ìŠ¤íŠ¸ ìœ ì§€Â·ì—°ì†ì ì¸ ëŒ€í™” íë¦„ |\n",
      "| **Indexes / Vector Stores** | í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•´ ë²¡í„° DBì— ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ê¸°ëŠ¥ | RAG(Retrievalâ€‘Augmented Generation) êµ¬í˜„ |\n",
      "| **Callbacks** | ì²´ì¸Â·ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì ì— ë¡œê¹…Â·ëª¨ë‹ˆí„°ë§Â·ë””ë²„ê¹…ì„ ìœ„í•œ í›… | íŠ¸ë ˆì´ìŠ¤Â·ì„±ëŠ¥ ë¶„ì„Â·ì—ëŸ¬ ì²˜ë¦¬ |\n",
      "| **Integrations** | OpenAI, Anthropic, Cohere, HuggingFace, Azure ë“± ë‹¤ì–‘í•œ LLM ì œê³µìì™€ì˜ ì—°ê²° | ëª¨ë¸ êµì²´Â·ë©€í‹°â€‘ëª¨ë¸ ì „ëµ |\n",
      "| **Utilities** | í…ìŠ¤íŠ¸ íŒŒì‹±, JSON ë³€í™˜, ë°ì´í„° ì •ì œ ë“± ë³´ì¡° í•¨ìˆ˜ë“¤ | ì „ì²˜ë¦¬Â·í›„ì²˜ë¦¬ ì‘ì—… ê°„ì†Œí™” |\n",
      "\n",
      "---\n",
      "\n",
      "## ì™œ LangChainì„ ì‚¬ìš©í•˜ë‚˜ìš”?\n",
      "\n",
      "1. **ì¬ì‚¬ìš©ì„±**: `Chain`Â·`PromptTemplate`Â·`Memory` ë“±ì„ ëª¨ë“ˆí™”í•´ ì—¬ëŸ¬ í”„ë¡œì íŠ¸ì—ì„œ ê·¸ëŒ€ë¡œ í™œìš© ê°€ëŠ¥.\n",
      "2. **ë³µí•© ë¡œì§ êµ¬í˜„**: LLM í˜¸ì¶œë§Œìœ¼ë¡œëŠ” ì–´ë ¤ìš´ â€œê²€ìƒ‰ â†’ ìš”ì•½ â†’ ì§ˆë¬¸ ìƒì„±â€ ê°™ì€ ë‹¤ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ë¥¼ ê°„ë‹¨íˆ ì •ì˜.\n",
      "3. **íˆ´ ì—°ë™**: `Agent`ë¥¼ í†µí•´ ì›¹ ê²€ìƒ‰, ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬, ì™¸ë¶€ API í˜¸ì¶œ ë“±ì„ LLMì´ ìë™ìœ¼ë¡œ ì„ íƒÂ·ì‹¤í–‰.\n",
      "4. **ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬**: `Memory`ë¥¼ ì´ìš©í•´ ëŒ€í™” íë¦„ì„ ìœ ì§€í•˜ê³ , ì´ì „ ë‹µë³€ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ë‹µë³€ì„ ìƒì„±.\n",
      "5. **RAG ì§€ì›**: ë²¡í„° ìŠ¤í† ì–´ì™€ ê²°í•©í•´ ì‚¬ì „ ì§€ì‹(ë¬¸ì„œ, FAQ ë“±)ì„ LLMì— ë³´ê°•í•´ ì •í™•ë„Â·ì‹ ë¢°ì„± í–¥ìƒ.\n",
      "6. **ë””ë²„ê¹…Â·ëª¨ë‹ˆí„°ë§**: ì½œë°± ì‹œìŠ¤í…œìœ¼ë¡œ ì‹¤í–‰ íë¦„ì„ ì¶”ì í•˜ê³ , í† í° ì‚¬ìš©ëŸ‰Â·ì‘ë‹µ ì‹œê°„ ë“±ì„ ë¡œê¹….\n",
      "\n",
      "---\n",
      "\n",
      "## ê¸°ë³¸ ì‚¬ìš© íë¦„ (Python ì˜ˆì‹œ)\n",
      "\n",
      "```python\n",
      "from langchain import OpenAI, PromptTemplate, LLMChain, VectorDBQA\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.document_loaders import TextLoader\n",
      "\n",
      "# 1ï¸âƒ£ LLM ì´ˆê¸°í™”\n",
      "llm = OpenAI(model_name=\"gpt-4o-mini\", temperature=0.2)\n",
      "\n",
      "# 2ï¸âƒ£ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
      "template = \"\"\"ë‹¤ìŒ ë¬¸ì„œë¥¼ ìš”ì•½í•´ ì£¼ì„¸ìš”:\n",
      "{context}\n",
      "\n",
      "ìš”ì•½: \"\"\"\n",
      "prompt = PromptTemplate.from_template(template)\n",
      "\n",
      "# 3ï¸âƒ£ ì²´ì¸ ìƒì„± (LLM + í”„ë¡¬í”„íŠ¸)\n",
      "summarize_chain = LLMChain(llm=llm, prompt=prompt)\n",
      "\n",
      "# 4ï¸âƒ£ ë¬¸ì„œ ë¡œë“œ & ì„ë² ë”© â†’ ë²¡í„° DB ìƒì„± (RAGìš©)\n",
      "loader = TextLoader(\"data/knowledge.txt\")\n",
      "docs = loader.load()\n",
      "embeddings = OpenAIEmbeddings()\n",
      "vector_store = FAISS.from_documents(docs, embeddings)\n",
      "\n",
      "# 5ï¸âƒ£ Retrieval QA ì²´ì¸ (ê²€ìƒ‰ + ìš”ì•½)\n",
      "qa = VectorDBQA.from_chain_type(\n",
      "    llm=llm,\n",
      "    chain_type=\"stuff\",   # \"map_reduce\", \"refine\" ë“± ì„ íƒ ê°€ëŠ¥\n",
      "    vectorstore=vector_store,\n",
      "    return_source_documents=True,\n",
      ")\n",
      "\n",
      "# 6ï¸âƒ£ ì‹¤ì œ ì§ˆë¬¸ ìˆ˜í–‰\n",
      "query = \"ìš°ë¦¬ íšŒì‚¬ì˜ ë°ì´í„° ë³´ì•ˆ ì •ì±…ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\"\n",
      "answer = qa.run(query)\n",
      "print(answer)\n",
      "```\n",
      "\n",
      "ìœ„ ì˜ˆì‹œëŠ” **RAG**(Retrievalâ€‘Augmented Generation) íë¦„ì„ êµ¬ì„±í•œ ê°„ë‹¨í•œ ì½”ë“œì…ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## ì£¼ìš” í™œìš© ì‚¬ë¡€\n",
      "\n",
      "| ë¶„ì•¼ | êµ¬ì²´ì ì¸ ì˜ˆì‹œ |\n",
      "|------|---------------|\n",
      "| **ì±—ë´‡ / ê³ ê°ì§€ì›** | ë©”ëª¨ë¦¬ì™€ íˆ´ì„ ê²°í•©í•´ ì£¼ë¬¸ ì¡°íšŒÂ·í™˜ë¶ˆ ì²˜ë¦¬Â·FAQ ìë™ì‘ë‹µ |\n",
      "| **ë¬¸ì„œ ìš”ì•½Â·ê²€ìƒ‰** | ì‚¬ë‚´ ìœ„í‚¤Â·ë²•ë¥  ë¬¸ì„œÂ·ì—°êµ¬ ë…¼ë¬¸ì„ ë²¡í„° DBì— ì €ì¥í•˜ê³ , ìì—°ì–´ ì§ˆì˜ë¡œ ì¦‰ì‹œ ìš”ì•½Â·ì¸ìš© |\n",
      "| **ì½”ë“œ ìë™ ìƒì„±Â·ë¦¬íŒ©í„°ë§** | ì½”ë“œë² ì´ìŠ¤ë¥¼ ì¸ë±ì‹±í•˜ê³ , â€œì´ í•¨ìˆ˜ì˜ ë³µì¡ë„ë¥¼ ë‚®ì¶°ì¤˜â€ ê°™ì€ ëª…ë ¹ì„ ì‹¤í–‰ |\n",
      "| **ë°ì´í„° ë¶„ì„** | LLMì´ SQLì„ ìƒì„±Â·ì‹¤í–‰í•´ ë°ì´í„° ì‹œê°í™”Â·í†µê³„ ìš”ì•½ ë³´ê³ ì„œ ìë™ ìƒì„± |\n",
      "| **ì›Œí¬í”Œë¡œ ìë™í™”** | LLMì´ í”„ë¡œì íŠ¸ ê´€ë¦¬ íˆ´ APIë¥¼ í˜¸ì¶œí•´ í‹°ì¼“ ìƒì„±Â·ì—…ë°ì´íŠ¸Â·ì•Œë¦¼ ì „ì†¡ |\n",
      "| **êµìœ¡Â·í•™ìŠµ** | í•™ìƒ ì§ˆë¬¸ì— ë§ì¶¤í˜• ì„¤ëª…ì„ ì œê³µí•˜ê³ , êµì¬ë¥¼ ê²€ìƒ‰Â·ìš”ì•½Â·í€´ì¦ˆ ìƒì„± |\n",
      "\n",
      "---\n",
      "\n",
      "## ì‹œì‘í•˜ê¸° â€“ ì„¤ì¹˜ì™€ ê¸°ë³¸ ì„¤ì •\n",
      "\n",
      "```bash\n",
      "# pip (Python) ê¸°ì¤€\n",
      "pip install langchain openai faiss-cpu   # ê¸°ë³¸ íŒ¨í‚¤ì§€\n",
      "# í•„ìš” ì‹œ ì¶”ê°€\n",
      "pip install langchain-community        # ì™¸ë¶€ íˆ´Â·í†µí•© ëª¨ë“ˆ\n",
      "pip install chromadb                    # ë‹¤ë¥¸ ë²¡í„° DB ì˜µì…˜\n",
      "```\n",
      "\n",
      "```python\n",
      "import os\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
      "```\n",
      "\n",
      "> **Tip**: `langchain-community` íŒ¨í‚¤ì§€ëŠ” **ê²€ìƒ‰ ì—”ì§„(SerpAPI, DuckDuckGo), ë°ì´í„°ë² ì´ìŠ¤(SQLAlchemy), ì›¹ ìŠ¤í¬ë˜í•‘** ë“± ë‹¤ì–‘í•œ ì™¸ë¶€ íˆ´ í†µí•©ì„ ì œê³µí•˜ë‹ˆ, í”„ë¡œì íŠ¸ì— ë§ëŠ” í”ŒëŸ¬ê·¸ì¸ì„ ì°¾ì•„ ì„¤ì¹˜í•˜ë©´ ë©ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## ì£¼ìš” ë¦¬ì†ŒìŠ¤\n",
      "\n",
      "| ì¢…ë¥˜ | ë§í¬ |\n",
      "|------|------|\n",
      "| **ê³µì‹ ë¬¸ì„œ** | https://python.langchain.com/ |\n",
      "| **GitHub** | https://github.com/langchain-ai/langchain |\n",
      "| **íŠœí† ë¦¬ì–¼Â·ì˜ˆì œ** | LangChain Hub (https://smith.langchain.com/hub) â€“ í”„ë¦¬ë¹ŒíŠ¸ ì²´ì¸Â·í”„ë¡¬í”„íŠ¸ ê³µìœ  |\n",
      "| **ì»¤ë®¤ë‹ˆí‹°** | Discord, Slack, Reddit â€“ ì§ˆë¬¸Â·í”¼ë“œë°± í™œë°œ |\n",
      "| **êµìœ¡ ì½”ìŠ¤** | LangChain Academy, Coursera, Udemy ë“±ì—ì„œ ì œê³µë˜ëŠ” ì‹¤ì „ í”„ë¡œì íŠ¸ ê¸°ë°˜ ê°•ì¢Œ |\n",
      "\n",
      "---\n",
      "\n",
      "## ë§ˆë¬´ë¦¬\n",
      "\n",
      "- **LangChain**ì€ LLMì„ **ë‹¨ì¼ í•¨ìˆ˜**ê°€ ì•„ë‹ˆë¼ **ë³µí•© ì‹œìŠ¤í…œ**ì˜ í•œ êµ¬ì„± ìš”ì†Œë¡œ ë‹¤ë£¨ê²Œ í•´ ì£¼ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
      "- **Chains**ì™€ **Agents**ë¥¼ í™œìš©í•˜ë©´ â€œLLMì´ ì§ì ‘ íˆ´ì„ ì„ íƒÂ·ì‹¤í–‰â€í•˜ëŠ” ì§€ëŠ¥í˜• ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì†ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- **Memory**ì™€ **Vector Store**ë¥¼ ê²°í•©í•˜ë©´ ëŒ€í™”ì˜ ì—°ì†ì„± ë° ì‚¬ì „ ì§€ì‹ ë³´ê°•(RAG)ê¹Œì§€ êµ¬í˜„ ê°€ëŠ¥í•˜ë¯€ë¡œ, ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤Â·ì—”í„°í”„ë¼ì´ì¦ˆ í™˜ê²½ì— ì ìš©í•˜ê¸°ì— ë§¤ìš° ì í•©í•©ë‹ˆë‹¤.\n",
      "\n",
      "ê¶ê¸ˆí•œ ì ì´ ìˆê±°ë‚˜ êµ¬ì²´ì ì¸ êµ¬í˜„ ì˜ˆì œê°€ í•„ìš”í•˜ë©´ ì–¸ì œë“  ì•Œë ¤ ì£¼ì„¸ìš”! ğŸš€\n"
     ]
    }
   ],
   "source": [
    "print(type(response))\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
