{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8fa7ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello LangChain\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello LangChain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83669776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb097a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-pr\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "#load_dotenv(dotenv_path='.env')\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#print(OPENAI_API_KEY[:10])\n",
    "print(OPENAI_API_KEY[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef09a323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 개발자입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# prompt + llm + output \n",
    "\n",
    "# prompt: (쉽게 말해) AI에게 하는 질문\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"당신은 개발자입니다.\") , # AI에게 역할을 주는 것.\n",
    "     (\"user\", \"{input}\") ]\n",
    ")\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6c47481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "System: 당신은 개발자입니다.\n",
      "Human: 파이썬은 무엇인가요? 자세하게 설명해주세요\n"
     ]
    }
   ],
   "source": [
    "prompt_text = prompt.format(input=\"파이썬은 무엇인가요? 자세하게 설명해주세요\")\n",
    "print(type(prompt_text))\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c61ba15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_openai.chat_models.base.ChatOpenAI'>\n",
      "client=<openai.resources.chat.completions.completions.Completions object at 0x10f7439a0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10f7ec280> root_client=<openai.OpenAI object at 0x10f743e20> root_async_client=<openai.AsyncOpenAI object at 0x118390eb0> model_name='openai/gpt-oss-120b' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "#llm = ChatOpenAI(api_key=OPENAI_API_KEY, model_name=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# Groq는 ChatGPT와 호환 -> ChatGPT 모델 사용 가능\n",
    "# Groq API를 사용하는 ChatOpenAI 인스턴스 생성 (모델 가져와서 생성)\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    #model=\"meta-llama/llama-4-scout-17b-16e-instruct\", # 사용 모델: llama-4-scout\n",
    "    #model=\"moonshotai/kimi-k2-instruct-0905\", # 사용 모델: kimi-k2\n",
    "    model=\"openai/gpt-oss-120b\", # 사용 모델: gpt-oss-120b\n",
    "    temperature=0.7\n",
    ")\n",
    "print(type(llm)) # 타입: ChatOpenAI\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c3ff864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오류 발생: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n"
     ]
    }
   ],
   "source": [
    "# 생성한 모델에 질문하기(prompt 입력)\n",
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    print(type(response))\n",
    "    print(\"응답:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfc67af",
   "metadata": {},
   "source": [
    "# LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2e4a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd77300d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "first=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 개발자입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]) middle=[ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x10f7439a0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10f7ec280>, root_client=<openai.OpenAI object at 0x10f743e20>, root_async_client=<openai.AsyncOpenAI object at 0x118390eb0>, model_name='openai/gpt-oss-120b', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')] last=StrOutputParser()\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "print(type(chain))\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ee6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"input\":\"LangChain이 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e647dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "## LangChain이란?\n",
      "\n",
      "**LangChain**은 **대규모 언어 모델(LLM)**을 활용한 **애플리케이션**을 보다 쉽게 설계·구현·배포할 수 있도록 도와주는 **프레임워크**이자 **툴킷**입니다. 주로 파이썬(Python)과 자바스크립트(TypeScript/Node.js) 환경에서 사용되며, LLM을 단순히 “텍스트를 생성”하는 수준을 넘어 **복합적인 워크플로우**와 **외부 시스템 연동**까지 포괄하는 애플리케이션을 만들 때 유용합니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 핵심 개념 & 구성 요소\n",
      "\n",
      "| 구성 요소 | 설명 | 주요 역할 |\n",
      "|-----------|------|-----------|\n",
      "| **Chains** | 여러 LLM 호출·프롬프트·툴을 순차·조건부로 연결한 파이프라인 | 복잡한 로직을 단계별로 나누어 재사용 가능 |\n",
      "| **Agents** | 목표(Goal)와 툴(tool) 집합을 받아, LLM이 “어떤 툴을 언제 사용할지” 스스로 판단하도록 하는 구조 | 동적 의사결정·툴 호출(검색, DB, API 등) |\n",
      "| **Prompt Templates** | 프롬프트를 변수화·재사용 가능하게 만든 템플릿 | 일관된 프롬프트 관리·다양한 입력에 맞춤 |\n",
      "| **Memory** | 대화·세션의 상태를 저장·조회하는 메커니즘 | 컨텍스트 유지·연속적인 대화 흐름 |\n",
      "| **Indexes / Vector Stores** | 텍스트를 임베딩해 벡터 DB에 저장하고 검색하는 기능 | RAG(Retrieval‑Augmented Generation) 구현 |\n",
      "| **Callbacks** | 체인·에이전트 실행 시점에 로깅·모니터링·디버깅을 위한 훅 | 트레이스·성능 분석·에러 처리 |\n",
      "| **Integrations** | OpenAI, Anthropic, Cohere, HuggingFace, Azure 등 다양한 LLM 제공자와의 연결 | 모델 교체·멀티‑모델 전략 |\n",
      "| **Utilities** | 텍스트 파싱, JSON 변환, 데이터 정제 등 보조 함수들 | 전처리·후처리 작업 간소화 |\n",
      "\n",
      "---\n",
      "\n",
      "## 왜 LangChain을 사용하나요?\n",
      "\n",
      "1. **재사용성**: `Chain`·`PromptTemplate`·`Memory` 등을 모듈화해 여러 프로젝트에서 그대로 활용 가능.\n",
      "2. **복합 로직 구현**: LLM 호출만으로는 어려운 “검색 → 요약 → 질문 생성” 같은 다단계 프로세스를 간단히 정의.\n",
      "3. **툴 연동**: `Agent`를 통해 웹 검색, 데이터베이스 쿼리, 외부 API 호출 등을 LLM이 자동으로 선택·실행.\n",
      "4. **컨텍스트 관리**: `Memory`를 이용해 대화 흐름을 유지하고, 이전 답변을 기반으로 새로운 답변을 생성.\n",
      "5. **RAG 지원**: 벡터 스토어와 결합해 사전 지식(문서, FAQ 등)을 LLM에 보강해 정확도·신뢰성 향상.\n",
      "6. **디버깅·모니터링**: 콜백 시스템으로 실행 흐름을 추적하고, 토큰 사용량·응답 시간 등을 로깅.\n",
      "\n",
      "---\n",
      "\n",
      "## 기본 사용 흐름 (Python 예시)\n",
      "\n",
      "```python\n",
      "from langchain import OpenAI, PromptTemplate, LLMChain, VectorDBQA\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.document_loaders import TextLoader\n",
      "\n",
      "# 1️⃣ LLM 초기화\n",
      "llm = OpenAI(model_name=\"gpt-4o-mini\", temperature=0.2)\n",
      "\n",
      "# 2️⃣ 프롬프트 템플릿 정의\n",
      "template = \"\"\"다음 문서를 요약해 주세요:\n",
      "{context}\n",
      "\n",
      "요약: \"\"\"\n",
      "prompt = PromptTemplate.from_template(template)\n",
      "\n",
      "# 3️⃣ 체인 생성 (LLM + 프롬프트)\n",
      "summarize_chain = LLMChain(llm=llm, prompt=prompt)\n",
      "\n",
      "# 4️⃣ 문서 로드 & 임베딩 → 벡터 DB 생성 (RAG용)\n",
      "loader = TextLoader(\"data/knowledge.txt\")\n",
      "docs = loader.load()\n",
      "embeddings = OpenAIEmbeddings()\n",
      "vector_store = FAISS.from_documents(docs, embeddings)\n",
      "\n",
      "# 5️⃣ Retrieval QA 체인 (검색 + 요약)\n",
      "qa = VectorDBQA.from_chain_type(\n",
      "    llm=llm,\n",
      "    chain_type=\"stuff\",   # \"map_reduce\", \"refine\" 등 선택 가능\n",
      "    vectorstore=vector_store,\n",
      "    return_source_documents=True,\n",
      ")\n",
      "\n",
      "# 6️⃣ 실제 질문 수행\n",
      "query = \"우리 회사의 데이터 보안 정책은 어떻게 되나요?\"\n",
      "answer = qa.run(query)\n",
      "print(answer)\n",
      "```\n",
      "\n",
      "위 예시는 **RAG**(Retrieval‑Augmented Generation) 흐름을 구성한 간단한 코드입니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 주요 활용 사례\n",
      "\n",
      "| 분야 | 구체적인 예시 |\n",
      "|------|---------------|\n",
      "| **챗봇 / 고객지원** | 메모리와 툴을 결합해 주문 조회·환불 처리·FAQ 자동응답 |\n",
      "| **문서 요약·검색** | 사내 위키·법률 문서·연구 논문을 벡터 DB에 저장하고, 자연어 질의로 즉시 요약·인용 |\n",
      "| **코드 자동 생성·리팩터링** | 코드베이스를 인덱싱하고, “이 함수의 복잡도를 낮춰줘” 같은 명령을 실행 |\n",
      "| **데이터 분석** | LLM이 SQL을 생성·실행해 데이터 시각화·통계 요약 보고서 자동 생성 |\n",
      "| **워크플로 자동화** | LLM이 프로젝트 관리 툴 API를 호출해 티켓 생성·업데이트·알림 전송 |\n",
      "| **교육·학습** | 학생 질문에 맞춤형 설명을 제공하고, 교재를 검색·요약·퀴즈 생성 |\n",
      "\n",
      "---\n",
      "\n",
      "## 시작하기 – 설치와 기본 설정\n",
      "\n",
      "```bash\n",
      "# pip (Python) 기준\n",
      "pip install langchain openai faiss-cpu   # 기본 패키지\n",
      "# 필요 시 추가\n",
      "pip install langchain-community        # 외부 툴·통합 모듈\n",
      "pip install chromadb                    # 다른 벡터 DB 옵션\n",
      "```\n",
      "\n",
      "```python\n",
      "import os\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
      "```\n",
      "\n",
      "> **Tip**: `langchain-community` 패키지는 **검색 엔진(SerpAPI, DuckDuckGo), 데이터베이스(SQLAlchemy), 웹 스크래핑** 등 다양한 외부 툴 통합을 제공하니, 프로젝트에 맞는 플러그인을 찾아 설치하면 됩니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 주요 리소스\n",
      "\n",
      "| 종류 | 링크 |\n",
      "|------|------|\n",
      "| **공식 문서** | https://python.langchain.com/ |\n",
      "| **GitHub** | https://github.com/langchain-ai/langchain |\n",
      "| **튜토리얼·예제** | LangChain Hub (https://smith.langchain.com/hub) – 프리빌트 체인·프롬프트 공유 |\n",
      "| **커뮤니티** | Discord, Slack, Reddit – 질문·피드백 활발 |\n",
      "| **교육 코스** | LangChain Academy, Coursera, Udemy 등에서 제공되는 실전 프로젝트 기반 강좌 |\n",
      "\n",
      "---\n",
      "\n",
      "## 마무리\n",
      "\n",
      "- **LangChain**은 LLM을 **단일 함수**가 아니라 **복합 시스템**의 한 구성 요소로 다루게 해 주는 프레임워크입니다.\n",
      "- **Chains**와 **Agents**를 활용하면 “LLM이 직접 툴을 선택·실행”하는 지능형 애플리케이션을 손쉽게 만들 수 있습니다.\n",
      "- **Memory**와 **Vector Store**를 결합하면 대화의 연속성 및 사전 지식 보강(RAG)까지 구현 가능하므로, 실제 비즈니스·엔터프라이즈 환경에 적용하기에 매우 적합합니다.\n",
      "\n",
      "궁금한 점이 있거나 구체적인 구현 예제가 필요하면 언제든 알려 주세요! 🚀\n"
     ]
    }
   ],
   "source": [
    "print(type(response))\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
